{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Viadot A simple data ingestion library to guide data flows from some places to other places Getting Data from a Source viadot supports few sources. For instance, the UK Carbon Intensity API does not require credentials. from viadot.sources.uk_carbon_intensity import UKCarbonIntensity ukci = UKCarbonIntensity() ukci.query(\"/intensity\") ukci.to_df() The above code pulls the UK Carbon Insentity data from the external API to the local Pandas dataframe (df). Loading Data to a Source TODO Running tests run.sh docker exec -it viadot_testing bash cd tests/ && pytest . Running flows locally run.sh poetry shell FLOW_NAME=supermetrics_to_azure_sql; python -m viadot.flows.$FLOW_NAME Uploading pkg to PyPi Generate the requirements.txt file from poetry. poetry export -f requirements.txt --output requirements.txt --with-credentials --dev And then publish with poetry. poetry update poetry publish --build","title":"Viadot"},{"location":"#viadot","text":"A simple data ingestion library to guide data flows from some places to other places","title":"Viadot"},{"location":"#getting-data-from-a-source","text":"viadot supports few sources. For instance, the UK Carbon Intensity API does not require credentials. from viadot.sources.uk_carbon_intensity import UKCarbonIntensity ukci = UKCarbonIntensity() ukci.query(\"/intensity\") ukci.to_df() The above code pulls the UK Carbon Insentity data from the external API to the local Pandas dataframe (df).","title":"Getting Data from a Source"},{"location":"#loading-data-to-a-source","text":"TODO","title":"Loading Data to a Source"},{"location":"#running-tests","text":"run.sh docker exec -it viadot_testing bash cd tests/ && pytest .","title":"Running tests"},{"location":"#running-flows-locally","text":"run.sh poetry shell FLOW_NAME=supermetrics_to_azure_sql; python -m viadot.flows.$FLOW_NAME","title":"Running flows locally"},{"location":"#uploading-pkg-to-pypi","text":"Generate the requirements.txt file from poetry. poetry export -f requirements.txt --output requirements.txt --with-credentials --dev And then publish with poetry. poetry update poetry publish --build","title":"Uploading pkg to PyPi"},{"location":"home/","text":"Viadot A simple data ingestion library to guide data flows from some places to other places Structure This documentation is following the di\u00e1taxis framework. Getting Data from a Source viadot supports few sources. For instance, the UK Carbon Intensity API does not require credentials. from viadot.sources.uk_carbon_intensity import UKCarbonIntensity ukci = UKCarbonIntensity() ukci.query(\"/intensity\") ukci.to_df() The above code pulls the UK Carbon Insentity data from the external API to the local Pandas dataframe (df). Loading Data to a Source TODO Running tests run.sh docker exec -it viadot_testing bash cd tests/ && pytest . Running flows locally run.sh poetry shell FLOW_NAME=supermetrics_to_azure_sql; python -m viadot.flows.$FLOW_NAME Uploading pkg to PyPi Generate the requirements.txt file from poetry. poetry export -f requirements.txt --output requirements.txt --with-credentials --dev And then publish with poetry. poetry update poetry publish --build","title":"Home"},{"location":"home/#viadot","text":"A simple data ingestion library to guide data flows from some places to other places","title":"Viadot"},{"location":"home/#structure","text":"This documentation is following the di\u00e1taxis framework.","title":"Structure"},{"location":"home/#getting-data-from-a-source","text":"viadot supports few sources. For instance, the UK Carbon Intensity API does not require credentials. from viadot.sources.uk_carbon_intensity import UKCarbonIntensity ukci = UKCarbonIntensity() ukci.query(\"/intensity\") ukci.to_df() The above code pulls the UK Carbon Insentity data from the external API to the local Pandas dataframe (df).","title":"Getting Data from a Source"},{"location":"home/#loading-data-to-a-source","text":"TODO","title":"Loading Data to a Source"},{"location":"home/#running-tests","text":"run.sh docker exec -it viadot_testing bash cd tests/ && pytest .","title":"Running tests"},{"location":"home/#running-flows-locally","text":"run.sh poetry shell FLOW_NAME=supermetrics_to_azure_sql; python -m viadot.flows.$FLOW_NAME","title":"Running flows locally"},{"location":"home/#uploading-pkg-to-pypi","text":"Generate the requirements.txt file from poetry. poetry export -f requirements.txt --output requirements.txt --with-credentials --dev And then publish with poetry. poetry update poetry publish --build","title":"Uploading pkg to PyPi"},{"location":"howtos/config_file/","text":"Config File Credentials and other settings for various sources are stored in a file named credentials.json . A credential file needs to be written in json format. A typical credentials file looks like so: { \"SUPERMETRICS\": { \"API_KEY\": \"apikey from supermetrics\", \"USER\": \"user@gmail.com\", \"SOURCES\": { \"Google Ads\": { \"Accounts\": [ \"456\" ] } } }, \"AZURE_SQL\": { \"server\": \"server url\", \"db_name\": \"db name\", \"user\": \"user\", \"password\": \"db password\", \"driver\": \"driver\" } } In the above SUPERMETRICS and AZURE_SQL are config_keys. These config settings are fed to the Supermetrics() or AzureSQL() Sources. For example, this is how to use the AZURE_SQL configuration stanza from the credentials file. # initiates the AzureSQL class with the AZURE_SQL configs azure_sql = AzureSQL(config_key=\"AZURE_SQL\") The above will pass all the configurations, including secrets like passwords, to the class. This avoids having to write secrets or configs in the code. Storing the file locally Currently only local files are supported. Make sure to store the file in the correct path. On Linux the path is /home/viadot/.config/credentials.json On Windows you need to create a .config folder with credentials.json inside the User folder C:\\Users\\<user>","title":"Config File"},{"location":"howtos/config_file/#config-file","text":"Credentials and other settings for various sources are stored in a file named credentials.json . A credential file needs to be written in json format. A typical credentials file looks like so: { \"SUPERMETRICS\": { \"API_KEY\": \"apikey from supermetrics\", \"USER\": \"user@gmail.com\", \"SOURCES\": { \"Google Ads\": { \"Accounts\": [ \"456\" ] } } }, \"AZURE_SQL\": { \"server\": \"server url\", \"db_name\": \"db name\", \"user\": \"user\", \"password\": \"db password\", \"driver\": \"driver\" } } In the above SUPERMETRICS and AZURE_SQL are config_keys. These config settings are fed to the Supermetrics() or AzureSQL() Sources. For example, this is how to use the AZURE_SQL configuration stanza from the credentials file. # initiates the AzureSQL class with the AZURE_SQL configs azure_sql = AzureSQL(config_key=\"AZURE_SQL\") The above will pass all the configurations, including secrets like passwords, to the class. This avoids having to write secrets or configs in the code.","title":"Config File"},{"location":"howtos/config_file/#storing-the-file-locally","text":"Currently only local files are supported. Make sure to store the file in the correct path. On Linux the path is /home/viadot/.config/credentials.json On Windows you need to create a .config folder with credentials.json inside the User folder C:\\Users\\<user>","title":"Storing the file locally"},{"location":"howtos/flows/","text":"Using flows Viadot flows subclass Prefect Flow class. We take this class, specify the tasks, and build the flow using the parameters provided at initialization time. See Prefect Flow documentation for more information. For instance, a S3 to Redshift flow would include the tasks necessary to insert S3 files into Redshift, and automate the generation of the flow. You only need to pass the required parameters. For examples, check out the examples folder. Writing flows For now, see the existing flows in viadot/flows .","title":"Flows"},{"location":"howtos/flows/#using-flows","text":"Viadot flows subclass Prefect Flow class. We take this class, specify the tasks, and build the flow using the parameters provided at initialization time. See Prefect Flow documentation for more information. For instance, a S3 to Redshift flow would include the tasks necessary to insert S3 files into Redshift, and automate the generation of the flow. You only need to pass the required parameters. For examples, check out the examples folder.","title":"Using flows"},{"location":"howtos/flows/#writing-flows","text":"For now, see the existing flows in viadot/flows .","title":"Writing flows"},{"location":"howtos/insert/","text":"Loading Data to a Source For creating SQlite database and uploading table with data use Insert class. Using of run function on Insert class instance create database in directory specified by path parameter and will complete sql table by pandas DataFrame. from viadot.tasks.sqlite_tasks import Insert insert = Insert() insert.run(table_name=TABLE_NAME, dtypes=dtypes, db_path=database_path, df=df, if_exists=\"replace\")","title":"Insert data"},{"location":"howtos/insert/#loading-data-to-a-source","text":"For creating SQlite database and uploading table with data use Insert class. Using of run function on Insert class instance create database in directory specified by path parameter and will complete sql table by pandas DataFrame. from viadot.tasks.sqlite_tasks import Insert insert = Insert() insert.run(table_name=TABLE_NAME, dtypes=dtypes, db_path=database_path, df=df, if_exists=\"replace\")","title":"Loading Data to a Source"},{"location":"howtos/sqltodf/","text":"Loading Data to a Source For getting pandas DataFrame from sql query use SQLtoDF class. During connection to an SQLite database, SQLite automatically creates the new database in directory specified by path parameter. To return a query, use the sql file, passing its path to the class constructor. Using of run function on SQLtoDF class instance return DataFrame object. from viadot.tasks.sqlite_tasks import SQLtoDF sql = SQLtoDF(db_path=database_path, sql_path=sql_path) df_from_task = sql.run()","title":"SQLtoDF"},{"location":"howtos/sqltodf/#loading-data-to-a-source","text":"For getting pandas DataFrame from sql query use SQLtoDF class. During connection to an SQLite database, SQLite automatically creates the new database in directory specified by path parameter. To return a query, use the sql file, passing its path to the class constructor. Using of run function on SQLtoDF class instance return DataFrame object. from viadot.tasks.sqlite_tasks import SQLtoDF sql = SQLtoDF(db_path=database_path, sql_path=sql_path) df_from_task = sql.run()","title":"Loading Data to a Source"},{"location":"howtos/uk_carbon_intensity/","text":"Using StatsToCSV and StatsToExcel class For downloading UK Carbon Intensity Statistics to a csv or excel file, use StatsToCSV or StatsToExcel class. Initialize the StatsToCSV and StatsToExcel # initiates the StatsToCSV and StatsToExcel class with the UK Carbon Intensity statistics from viadot.tasks.open_apis.uk_carbon_intensity import StatsToExcel, StatsToCSV statistics_csv = StatsToCSV() statistics_excel = StatsToExcel() Generation of csv or excel file Next, run task with csv or excel file generation. statistics_csv.run(\"out.csv\") statistics_excel.run(\"out.xlsx\") This function require parameter filename. Optional, use days_back parameter to download statistics for this time. By default it is 10 days, but You can use for example 30, run(filename, 30) . Running tests To run tests of StatsToCSV and StatsToExcel class go into tests/prefect directory and run command: pytest","title":"Get Excel file"},{"location":"howtos/uk_carbon_intensity/#using-statstocsv-and-statstoexcel-class","text":"For downloading UK Carbon Intensity Statistics to a csv or excel file, use StatsToCSV or StatsToExcel class.","title":"Using StatsToCSV and StatsToExcel class"},{"location":"howtos/uk_carbon_intensity/#initialize-the-statstocsv-and-statstoexcel","text":"# initiates the StatsToCSV and StatsToExcel class with the UK Carbon Intensity statistics from viadot.tasks.open_apis.uk_carbon_intensity import StatsToExcel, StatsToCSV statistics_csv = StatsToCSV() statistics_excel = StatsToExcel()","title":"Initialize the StatsToCSV and StatsToExcel"},{"location":"howtos/uk_carbon_intensity/#generation-of-csv-or-excel-file","text":"Next, run task with csv or excel file generation. statistics_csv.run(\"out.csv\") statistics_excel.run(\"out.xlsx\") This function require parameter filename. Optional, use days_back parameter to download statistics for this time. By default it is 10 days, but You can use for example 30, run(filename, 30) .","title":"Generation of csv or excel file"},{"location":"howtos/uk_carbon_intensity/#running-tests","text":"To run tests of StatsToCSV and StatsToExcel class go into tests/prefect directory and run command: pytest","title":"Running tests"},{"location":"references/api_sources/","text":"API Sources viadot.sources.uk_carbon_intensity.UKCarbonIntensity Fetches data of Carbon Intensity of the UK Power Grid. Documentation for this source API is located at: https://carbon-intensity.github.io/api-definitions/#carbon-intensity-api-v2-0-0 Parameters api_url : str, optional The URL endpoint to call, by default None to_df ( self , if_empty = 'warn' ) Returns a pandas DataFrame with flattened data Returns: Type Description pandas.DataFrame A Pandas DataFrame Source code in viadot/sources/uk_carbon_intensity.py def to_df ( self , if_empty : str = \"warn\" ): \"\"\"Returns a pandas DataFrame with flattened data Returns: pandas.DataFrame: A Pandas DataFrame \"\"\" from_ = [] to = [] forecast = [] actual = [] max_ = [] average = [] min_ = [] index = [] json_data = self . to_json () if not json_data : self . _handle_if_empty ( if_empty ) for row in json_data [ \"data\" ]: from_ . append ( row [ \"from\" ]) to . append ( row [ \"to\" ]) index . append ( row [ \"intensity\" ][ \"index\" ]) try : forecast . append ( row [ \"intensity\" ][ \"forecast\" ]) actual . append ( row [ \"intensity\" ][ \"actual\" ]) df = pd . DataFrame ( { \"from\" : from_ , \"to\" : to , \"forecast\" : forecast , \"actual\" : actual , \"index\" : index , } ) except KeyError : max_ . append ( row [ \"intensity\" ][ \"max\" ]) average . append ( row [ \"intensity\" ][ \"average\" ]) min_ . append ( row [ \"intensity\" ][ \"min\" ]) df = pd . DataFrame ( { \"from\" : from_ , \"to\" : to , \"max\" : max_ , \"average\" : average , \"min\" : min_ , } ) return df viadot.sources.supermetrics.Supermetrics A class implementing the Supermetrics API. Documentation for this API is located at: https://supermetrics.com/docs/product-api-getting-started/ Usage limits: https://supermetrics.com/docs/product-api-usage-limits/ Parameters query_params : Dict[str, Any], optional The parameters to pass to the GET query. See https://supermetrics.com/docs/product-api-get-data/ for full specification, by default None to_df ( self , if_empty = 'warn' ) Download data into a pandas DataFrame. Note that Supermetric can calculate some fields on the fly and alias them in the returned result. For example, if the query requests the position field, Supermetric may return an Average position caclulated field. For this reason we take columns names from the actual results rather than from input fields. Parameters: Name Type Description Default if_empty str What to do if query returned no data. Defaults to \"warn\". 'warn' Returns: Type Description DataFrame pd.DataFrame: the DataFrame containing query results Source code in viadot/sources/supermetrics.py def to_df ( self , if_empty : str = \"warn\" ) -> pd . DataFrame : \"\"\"Download data into a pandas DataFrame. Note that Supermetric can calculate some fields on the fly and alias them in the returned result. For example, if the query requests the `position` field, Supermetric may return an `Average position` caclulated field. For this reason we take columns names from the actual results rather than from input fields. Args: if_empty (str, optional): What to do if query returned no data. Defaults to \"warn\". Returns: pd.DataFrame: the DataFrame containing query results \"\"\" columns = self . _get_col_names () data = self . to_json ()[ \"data\" ] if data : df = pd . DataFrame ( data [ 1 :], columns = columns ) . replace ( \"\" , np . nan ) else : df = pd . DataFrame ( columns = columns ) if df . empty : self . _handle_if_empty ( if_empty ) return df to_json ( self , timeout = ( 3.05 , 1800 )) Download query results to a dictionary. Note that Supermetrics API will sometimes hang and not return any error message, so we're adding a timeout to GET. See requests docs for an explanation of why this timeout value will work on long-running queries but fail fast on connection issues. Source code in viadot/sources/supermetrics.py def to_json ( self , timeout = ( 3.05 , 60 * 30 )) -> Dict [ str , Any ]: \"\"\"Download query results to a dictionary. Note that Supermetrics API will sometimes hang and not return any error message, so we're adding a timeout to GET. See [requests docs](https://docs.python-requests.org/en/master/user/advanced/#timeouts) for an explanation of why this timeout value will work on long-running queries but fail fast on connection issues. \"\"\" if not self . query_params : raise ValueError ( \"Please build the query first\" ) params = { \"json\" : json . dumps ( self . query_params )} headers = { \"Authorization\" : f 'Bearer { self . credentials [ \"API_KEY\" ] } ' } try : session = requests . Session () retry_strategy = Retry ( total = 3 , status_forcelist = [ 429 , 500 , 502 , 503 , 504 ], backoff_factor = 1 ) adapter = HTTPAdapter ( max_retries = retry_strategy ) session . mount ( \"http://\" , adapter ) session . mount ( \"https://\" , adapter ) response = session . get ( self . API_ENDPOINT , params = params , headers = headers , timeout = timeout ) response . raise_for_status () except ReadTimeout as e : msg = \"The connection was successful, \" msg += f \"however the API call to { self . API_ENDPOINT } timed out after { timeout [ 1 ] } s \" msg += \"while waiting for the server to return data.\" raise APIError ( msg ) except HTTPError as e : raise APIError ( f \"The API call to { self . API_ENDPOINT } failed. \" \"Perhaps your account credentials need to be refreshed?\" , ) from e except ( ConnectionError , Timeout ) as e : raise APIError ( f \"The API call to { self . API_ENDPOINT } failed due to connection issues.\" ) from e except ProtocolError as e : raise APIError ( f \"Did not receive any reponse for the API call to { self . API_ENDPOINT } .\" ) except Exception as e : raise APIError ( \"Unknown error.\" ) from e return response . json ()","title":"API Sources"},{"location":"references/api_sources/#api-sources","text":"","title":"API Sources"},{"location":"references/api_sources/#viadot.sources.uk_carbon_intensity.UKCarbonIntensity","text":"Fetches data of Carbon Intensity of the UK Power Grid. Documentation for this source API is located at: https://carbon-intensity.github.io/api-definitions/#carbon-intensity-api-v2-0-0","title":"UKCarbonIntensity"},{"location":"references/api_sources/#viadot.sources.uk_carbon_intensity.UKCarbonIntensity--parameters","text":"api_url : str, optional The URL endpoint to call, by default None","title":"Parameters"},{"location":"references/api_sources/#viadot.sources.uk_carbon_intensity.UKCarbonIntensity.to_df","text":"Returns a pandas DataFrame with flattened data Returns: Type Description pandas.DataFrame A Pandas DataFrame Source code in viadot/sources/uk_carbon_intensity.py def to_df ( self , if_empty : str = \"warn\" ): \"\"\"Returns a pandas DataFrame with flattened data Returns: pandas.DataFrame: A Pandas DataFrame \"\"\" from_ = [] to = [] forecast = [] actual = [] max_ = [] average = [] min_ = [] index = [] json_data = self . to_json () if not json_data : self . _handle_if_empty ( if_empty ) for row in json_data [ \"data\" ]: from_ . append ( row [ \"from\" ]) to . append ( row [ \"to\" ]) index . append ( row [ \"intensity\" ][ \"index\" ]) try : forecast . append ( row [ \"intensity\" ][ \"forecast\" ]) actual . append ( row [ \"intensity\" ][ \"actual\" ]) df = pd . DataFrame ( { \"from\" : from_ , \"to\" : to , \"forecast\" : forecast , \"actual\" : actual , \"index\" : index , } ) except KeyError : max_ . append ( row [ \"intensity\" ][ \"max\" ]) average . append ( row [ \"intensity\" ][ \"average\" ]) min_ . append ( row [ \"intensity\" ][ \"min\" ]) df = pd . DataFrame ( { \"from\" : from_ , \"to\" : to , \"max\" : max_ , \"average\" : average , \"min\" : min_ , } ) return df","title":"to_df()"},{"location":"references/api_sources/#viadot.sources.supermetrics.Supermetrics","text":"A class implementing the Supermetrics API. Documentation for this API is located at: https://supermetrics.com/docs/product-api-getting-started/ Usage limits: https://supermetrics.com/docs/product-api-usage-limits/","title":"Supermetrics"},{"location":"references/api_sources/#viadot.sources.supermetrics.Supermetrics--parameters","text":"query_params : Dict[str, Any], optional The parameters to pass to the GET query. See https://supermetrics.com/docs/product-api-get-data/ for full specification, by default None","title":"Parameters"},{"location":"references/api_sources/#viadot.sources.supermetrics.Supermetrics.to_df","text":"Download data into a pandas DataFrame. Note that Supermetric can calculate some fields on the fly and alias them in the returned result. For example, if the query requests the position field, Supermetric may return an Average position caclulated field. For this reason we take columns names from the actual results rather than from input fields. Parameters: Name Type Description Default if_empty str What to do if query returned no data. Defaults to \"warn\". 'warn' Returns: Type Description DataFrame pd.DataFrame: the DataFrame containing query results Source code in viadot/sources/supermetrics.py def to_df ( self , if_empty : str = \"warn\" ) -> pd . DataFrame : \"\"\"Download data into a pandas DataFrame. Note that Supermetric can calculate some fields on the fly and alias them in the returned result. For example, if the query requests the `position` field, Supermetric may return an `Average position` caclulated field. For this reason we take columns names from the actual results rather than from input fields. Args: if_empty (str, optional): What to do if query returned no data. Defaults to \"warn\". Returns: pd.DataFrame: the DataFrame containing query results \"\"\" columns = self . _get_col_names () data = self . to_json ()[ \"data\" ] if data : df = pd . DataFrame ( data [ 1 :], columns = columns ) . replace ( \"\" , np . nan ) else : df = pd . DataFrame ( columns = columns ) if df . empty : self . _handle_if_empty ( if_empty ) return df","title":"to_df()"},{"location":"references/api_sources/#viadot.sources.supermetrics.Supermetrics.to_json","text":"Download query results to a dictionary. Note that Supermetrics API will sometimes hang and not return any error message, so we're adding a timeout to GET. See requests docs for an explanation of why this timeout value will work on long-running queries but fail fast on connection issues. Source code in viadot/sources/supermetrics.py def to_json ( self , timeout = ( 3.05 , 60 * 30 )) -> Dict [ str , Any ]: \"\"\"Download query results to a dictionary. Note that Supermetrics API will sometimes hang and not return any error message, so we're adding a timeout to GET. See [requests docs](https://docs.python-requests.org/en/master/user/advanced/#timeouts) for an explanation of why this timeout value will work on long-running queries but fail fast on connection issues. \"\"\" if not self . query_params : raise ValueError ( \"Please build the query first\" ) params = { \"json\" : json . dumps ( self . query_params )} headers = { \"Authorization\" : f 'Bearer { self . credentials [ \"API_KEY\" ] } ' } try : session = requests . Session () retry_strategy = Retry ( total = 3 , status_forcelist = [ 429 , 500 , 502 , 503 , 504 ], backoff_factor = 1 ) adapter = HTTPAdapter ( max_retries = retry_strategy ) session . mount ( \"http://\" , adapter ) session . mount ( \"https://\" , adapter ) response = session . get ( self . API_ENDPOINT , params = params , headers = headers , timeout = timeout ) response . raise_for_status () except ReadTimeout as e : msg = \"The connection was successful, \" msg += f \"however the API call to { self . API_ENDPOINT } timed out after { timeout [ 1 ] } s \" msg += \"while waiting for the server to return data.\" raise APIError ( msg ) except HTTPError as e : raise APIError ( f \"The API call to { self . API_ENDPOINT } failed. \" \"Perhaps your account credentials need to be refreshed?\" , ) from e except ( ConnectionError , Timeout ) as e : raise APIError ( f \"The API call to { self . API_ENDPOINT } failed due to connection issues.\" ) from e except ProtocolError as e : raise APIError ( f \"Did not receive any reponse for the API call to { self . API_ENDPOINT } .\" ) except Exception as e : raise APIError ( \"Unknown error.\" ) from e return response . json ()","title":"to_json()"},{"location":"references/sql_sources/","text":"SQL Sources viadot.sources.base.SQL con : Connection property readonly A singleton-like property for initiating a connection to the database. Returns: Type Description Connection pyodbc.Connection: database connection. conn_str : str property readonly Generate a connection string from params or config. Note that the user and password are escapedd with '{}' characters. Returns: Type Description str str: The ODBC connection string. __init__ ( self , driver = None , config_key = None , credentials = None , query_timeout = 3600 , * args , ** kwargs ) special A base SQL source class. Parameters: Name Type Description Default driver str The SQL driver to use. Defaults to None. None config_key str The key inside local config containing the config. None credentials str Credentials for the connection. Defaults to None. None query_timeout int The timeout for executed queries. Defaults to 1 hour. 3600 Source code in viadot/sources/base.py def __init__ ( self , driver : str = None , config_key : str = None , credentials : str = None , query_timeout : int = 60 * 60 , * args , ** kwargs , ): \"\"\"A base SQL source class. Args: driver (str, optional): The SQL driver to use. Defaults to None. config_key (str, optional): The key inside local config containing the config. User can choose to use this or pass credentials directly to the `credentials` parameter. Defaults to None. credentials (str, optional): Credentials for the connection. Defaults to None. query_timeout (int, optional): The timeout for executed queries. Defaults to 1 hour. \"\"\" self . query_timeout = query_timeout if config_key : config_credentials = local_config . get ( config_key ) credentials = config_credentials if config_key else credentials or {} if driver : credentials [ \"driver\" ] = driver super () . __init__ ( * args , credentials = credentials , ** kwargs ) self . _con = None create_table ( self , table , schema = None , dtypes = None , if_exists = 'fail' ) Create a table. Parameters: Name Type Description Default table str The destination table. Defaults to None. required schema str The destination schema. Defaults to None. None dtypes Dict[str, Any] [description]. Defaults to None. None if_exists Literal['fail', 'replace'] [description]. Defaults to \"fail\". 'fail' Returns: Type Description bool bool: Whether the operation was successful. Source code in viadot/sources/base.py def create_table ( self , table : str , schema : str = None , dtypes : Dict [ str , Any ] = None , if_exists : Literal [ \"fail\" , \"replace\" ] = \"fail\" , ) -> bool : \"\"\"Create a table. Args: table (str): The destination table. Defaults to None. schema (str, optional): The destination schema. Defaults to None. dtypes (Dict[str, Any], optional): [description]. Defaults to None. if_exists (Literal, optional): [description]. Defaults to \"fail\". Returns: bool: Whether the operation was successful. \"\"\" if schema is None : fqn = f \" { table } \" else : fqn = f \" { schema } . { table } \" indent = \" \" dtypes_rows = [ indent + f '\" { col } \"' + \" \" + dtype for col , dtype in dtypes . items () ] dtypes_formatted = \", \\n \" . join ( dtypes_rows ) create_table_sql = f \"CREATE TABLE { fqn } ( \\n { dtypes_formatted } \\n )\" if if_exists == \"replace\" : try : if schema == None : self . run ( f \"DROP TABLE { table } \" ) else : self . run ( f \"DROP TABLE { schema } . { table } \" ) except : pass self . run ( create_table_sql ) return True insert_into ( self , table , df ) Insert values from a pandas DataFrame into an existing database table. Parameters: Name Type Description Default table str table name required df DataFrame pandas dataframe required Returns: Type Description str str: The executed SQL insert query. Source code in viadot/sources/base.py def insert_into ( self , table : str , df : pd . DataFrame ) -> str : \"\"\"Insert values from a pandas DataFrame into an existing database table. Args: table (str): table name df (pd.DataFrame): pandas dataframe Returns: str: The executed SQL insert query. \"\"\" values = \"\" rows_count = df . shape [ 0 ] counter = 0 for row in df . values : counter += 1 out_row = \", \" . join ( map ( self . _sql_column , row )) comma = \", \\n \" if counter == rows_count : comma = \";\" out_row = f \"( { out_row } ) { comma } \" values += out_row columns = \", \" . join ( df . columns ) sql = f \"INSERT INTO { table } ( { columns } ) \\n VALUES { values } \" self . run ( sql ) return sql viadot.sources.azure_sql.AzureSQL create_external_database ( self , external_database_name , storage_account_name , container_name , sas_token , master_key_password , credential_name = None ) Create an external database. Used to eg. execute BULK INSERT or OPENROWSET queries. Parameters: Name Type Description Default external_database_name str The name of the extrnal source (db) to be created. required storage_account_name str The name of the Azure storage account. required container_name str The name of the container which should become the \"database\". required sas_token str The SAS token to be used as the credential. Note that the auth required master_key_password str The password for the database master key of your required credential_name str How to name the SAS credential. This is really an Azure None Source code in viadot/sources/azure_sql.py def create_external_database ( self , external_database_name : str , storage_account_name : str , container_name : str , sas_token : str , master_key_password : str , credential_name : str = None , ): \"\"\"Create an external database. Used to eg. execute BULK INSERT or OPENROWSET queries. Args: external_database_name (str): The name of the extrnal source (db) to be created. storage_account_name (str): The name of the Azure storage account. container_name (str): The name of the container which should become the \"database\". sas_token (str): The SAS token to be used as the credential. Note that the auth system in Azure is pretty broken and you might need to paste here your storage account's account key instead. master_key_password (str): The password for the database master key of your Azure SQL Database. credential_name (str): How to name the SAS credential. This is really an Azure internal thing and can be anything. By default '{external_database_name}_credential`. \"\"\" # stupid Microsoft thing if sas_token . startswith ( \"?\" ): sas_token = sas_token [ 1 :] if credential_name is None : credential_name = f \" { external_database_name } _credential\" create_master_key_sql = ( f \"CREATE MASTER KEY ENCRYPTION BY PASSWORD = { master_key_password } \" ) create_external_db_credential_sql = f \"\"\" CREATE DATABASE SCOPED CREDENTIAL { credential_name } WITH IDENTITY = 'SHARED ACCESS SIGNATURE' SECRET = ' { sas_token } '; \"\"\" create_external_db_sql = f \"\"\" CREATE EXTERNAL DATA SOURCE { external_database_name } WITH ( LOCATION = f'https:// { storage_account_name } .blob.core.windows.net/ { container_name } ', CREDENTIAL = { credential_name } ); \"\"\" self . run ( create_master_key_sql ) self . run ( create_external_db_credential_sql ) self . run ( create_external_db_sql ) exists ( self , table , schema = None ) Check whether a table exists. Parameters: Name Type Description Default table str The table to be checked. required schema str The schema whethe the table is located. Defaults to 'dbo'. None Returns: Type Description bool bool: Whether the table exists. Source code in viadot/sources/azure_sql.py def exists ( self , table : str , schema : str = None ) -> bool : \"\"\"Check whether a table exists. Args: table (str): The table to be checked. schema (str, optional): The schema whethe the table is located. Defaults to 'dbo'. Returns: bool: Whether the table exists. \"\"\" if not schema : schema = \"dbo\" list_table_info_query = f \"\"\" SELECT * FROM sys.tables t JOIN sys.schemas s ON t.schema_id = s.schema_id WHERE s.name = ' { schema } ' AND t.name = ' { table } ' \"\"\" exists = bool ( self . run ( list_table_info_query )) return exists viadot.sources.sqlite.SQLite A SQLite source Parameters: Name Type Description Default server str server string, usually localhost required db str the file path to the db e.g. /home/somedb.sqlite required conn_str property readonly Generate a connection string from params or config. Note that the user and password are escapedd with '{}' characters. Returns: Type Description str The ODBC connection string.","title":"SQL Sources"},{"location":"references/sql_sources/#sql-sources","text":"","title":"SQL Sources"},{"location":"references/sql_sources/#viadot.sources.base.SQL","text":"","title":"SQL"},{"location":"references/sql_sources/#viadot.sources.base.SQL.con","text":"A singleton-like property for initiating a connection to the database. Returns: Type Description Connection pyodbc.Connection: database connection.","title":"con"},{"location":"references/sql_sources/#viadot.sources.base.SQL.conn_str","text":"Generate a connection string from params or config. Note that the user and password are escapedd with '{}' characters. Returns: Type Description str str: The ODBC connection string.","title":"conn_str"},{"location":"references/sql_sources/#viadot.sources.base.SQL.__init__","text":"A base SQL source class. Parameters: Name Type Description Default driver str The SQL driver to use. Defaults to None. None config_key str The key inside local config containing the config. None credentials str Credentials for the connection. Defaults to None. None query_timeout int The timeout for executed queries. Defaults to 1 hour. 3600 Source code in viadot/sources/base.py def __init__ ( self , driver : str = None , config_key : str = None , credentials : str = None , query_timeout : int = 60 * 60 , * args , ** kwargs , ): \"\"\"A base SQL source class. Args: driver (str, optional): The SQL driver to use. Defaults to None. config_key (str, optional): The key inside local config containing the config. User can choose to use this or pass credentials directly to the `credentials` parameter. Defaults to None. credentials (str, optional): Credentials for the connection. Defaults to None. query_timeout (int, optional): The timeout for executed queries. Defaults to 1 hour. \"\"\" self . query_timeout = query_timeout if config_key : config_credentials = local_config . get ( config_key ) credentials = config_credentials if config_key else credentials or {} if driver : credentials [ \"driver\" ] = driver super () . __init__ ( * args , credentials = credentials , ** kwargs ) self . _con = None","title":"__init__()"},{"location":"references/sql_sources/#viadot.sources.base.SQL.create_table","text":"Create a table. Parameters: Name Type Description Default table str The destination table. Defaults to None. required schema str The destination schema. Defaults to None. None dtypes Dict[str, Any] [description]. Defaults to None. None if_exists Literal['fail', 'replace'] [description]. Defaults to \"fail\". 'fail' Returns: Type Description bool bool: Whether the operation was successful. Source code in viadot/sources/base.py def create_table ( self , table : str , schema : str = None , dtypes : Dict [ str , Any ] = None , if_exists : Literal [ \"fail\" , \"replace\" ] = \"fail\" , ) -> bool : \"\"\"Create a table. Args: table (str): The destination table. Defaults to None. schema (str, optional): The destination schema. Defaults to None. dtypes (Dict[str, Any], optional): [description]. Defaults to None. if_exists (Literal, optional): [description]. Defaults to \"fail\". Returns: bool: Whether the operation was successful. \"\"\" if schema is None : fqn = f \" { table } \" else : fqn = f \" { schema } . { table } \" indent = \" \" dtypes_rows = [ indent + f '\" { col } \"' + \" \" + dtype for col , dtype in dtypes . items () ] dtypes_formatted = \", \\n \" . join ( dtypes_rows ) create_table_sql = f \"CREATE TABLE { fqn } ( \\n { dtypes_formatted } \\n )\" if if_exists == \"replace\" : try : if schema == None : self . run ( f \"DROP TABLE { table } \" ) else : self . run ( f \"DROP TABLE { schema } . { table } \" ) except : pass self . run ( create_table_sql ) return True","title":"create_table()"},{"location":"references/sql_sources/#viadot.sources.base.SQL.insert_into","text":"Insert values from a pandas DataFrame into an existing database table. Parameters: Name Type Description Default table str table name required df DataFrame pandas dataframe required Returns: Type Description str str: The executed SQL insert query. Source code in viadot/sources/base.py def insert_into ( self , table : str , df : pd . DataFrame ) -> str : \"\"\"Insert values from a pandas DataFrame into an existing database table. Args: table (str): table name df (pd.DataFrame): pandas dataframe Returns: str: The executed SQL insert query. \"\"\" values = \"\" rows_count = df . shape [ 0 ] counter = 0 for row in df . values : counter += 1 out_row = \", \" . join ( map ( self . _sql_column , row )) comma = \", \\n \" if counter == rows_count : comma = \";\" out_row = f \"( { out_row } ) { comma } \" values += out_row columns = \", \" . join ( df . columns ) sql = f \"INSERT INTO { table } ( { columns } ) \\n VALUES { values } \" self . run ( sql ) return sql","title":"insert_into()"},{"location":"references/sql_sources/#viadot.sources.azure_sql.AzureSQL","text":"","title":"AzureSQL"},{"location":"references/sql_sources/#viadot.sources.azure_sql.AzureSQL.create_external_database","text":"Create an external database. Used to eg. execute BULK INSERT or OPENROWSET queries. Parameters: Name Type Description Default external_database_name str The name of the extrnal source (db) to be created. required storage_account_name str The name of the Azure storage account. required container_name str The name of the container which should become the \"database\". required sas_token str The SAS token to be used as the credential. Note that the auth required master_key_password str The password for the database master key of your required credential_name str How to name the SAS credential. This is really an Azure None Source code in viadot/sources/azure_sql.py def create_external_database ( self , external_database_name : str , storage_account_name : str , container_name : str , sas_token : str , master_key_password : str , credential_name : str = None , ): \"\"\"Create an external database. Used to eg. execute BULK INSERT or OPENROWSET queries. Args: external_database_name (str): The name of the extrnal source (db) to be created. storage_account_name (str): The name of the Azure storage account. container_name (str): The name of the container which should become the \"database\". sas_token (str): The SAS token to be used as the credential. Note that the auth system in Azure is pretty broken and you might need to paste here your storage account's account key instead. master_key_password (str): The password for the database master key of your Azure SQL Database. credential_name (str): How to name the SAS credential. This is really an Azure internal thing and can be anything. By default '{external_database_name}_credential`. \"\"\" # stupid Microsoft thing if sas_token . startswith ( \"?\" ): sas_token = sas_token [ 1 :] if credential_name is None : credential_name = f \" { external_database_name } _credential\" create_master_key_sql = ( f \"CREATE MASTER KEY ENCRYPTION BY PASSWORD = { master_key_password } \" ) create_external_db_credential_sql = f \"\"\" CREATE DATABASE SCOPED CREDENTIAL { credential_name } WITH IDENTITY = 'SHARED ACCESS SIGNATURE' SECRET = ' { sas_token } '; \"\"\" create_external_db_sql = f \"\"\" CREATE EXTERNAL DATA SOURCE { external_database_name } WITH ( LOCATION = f'https:// { storage_account_name } .blob.core.windows.net/ { container_name } ', CREDENTIAL = { credential_name } ); \"\"\" self . run ( create_master_key_sql ) self . run ( create_external_db_credential_sql ) self . run ( create_external_db_sql )","title":"create_external_database()"},{"location":"references/sql_sources/#viadot.sources.azure_sql.AzureSQL.exists","text":"Check whether a table exists. Parameters: Name Type Description Default table str The table to be checked. required schema str The schema whethe the table is located. Defaults to 'dbo'. None Returns: Type Description bool bool: Whether the table exists. Source code in viadot/sources/azure_sql.py def exists ( self , table : str , schema : str = None ) -> bool : \"\"\"Check whether a table exists. Args: table (str): The table to be checked. schema (str, optional): The schema whethe the table is located. Defaults to 'dbo'. Returns: bool: Whether the table exists. \"\"\" if not schema : schema = \"dbo\" list_table_info_query = f \"\"\" SELECT * FROM sys.tables t JOIN sys.schemas s ON t.schema_id = s.schema_id WHERE s.name = ' { schema } ' AND t.name = ' { table } ' \"\"\" exists = bool ( self . run ( list_table_info_query )) return exists","title":"exists()"},{"location":"references/sql_sources/#viadot.sources.sqlite.SQLite","text":"A SQLite source Parameters: Name Type Description Default server str server string, usually localhost required db str the file path to the db e.g. /home/somedb.sqlite required","title":"SQLite"},{"location":"references/sql_sources/#viadot.sources.sqlite.SQLite.conn_str","text":"Generate a connection string from params or config. Note that the user and password are escapedd with '{}' characters. Returns: Type Description str The ODBC connection string.","title":"conn_str"},{"location":"references/task_library/","text":"viadot.tasks.open_apis.uk_carbon_intensity.StatsToCSV A Prefect task for downloading UK Carbon Instensity Statistics (stats) to a csv file. __call__ ( self ) special Run the task. Parameters path : str Path of the csv file created or edited by this task days_back : int, optional How many days of stats to download in the csv. UK Carbon Intensity statistics are available for up to 30 days, by default 10 Source code in viadot/tasks/open_apis/uk_carbon_intensity.py def __call__ ( self ): \"\"\" Run the task. Parameters ---------- path : str Path of the csv file created or edited by this task days_back : int, optional How many days of stats to download in the csv. UK Carbon Intensity statistics are available for up to 30 days, by default 10 \"\"\" __init__ ( self , * args , ** kwargs ) special Generate the task. Source code in viadot/tasks/open_apis/uk_carbon_intensity.py def __init__ ( self , * args , ** kwargs ): \"\"\"Generate the task.\"\"\" super () . __init__ ( name = \"uk_carbon_intensity_stats_to_csv\" , * args , ** kwargs ) run ( self , path , days_back = 10 ) Run the task. Parameters path : str Path of the csv file created or edited by this task days_back : int, optional How many days of stats to download in the csv. UK Carbon Intensity statistics are available for up to 30 days, by default 10 Source code in viadot/tasks/open_apis/uk_carbon_intensity.py def run ( self , path : str , days_back : int = 10 ): \"\"\" Run the task. Parameters ---------- path : str Path of the csv file created or edited by this task days_back : int, optional How many days of stats to download in the csv. UK Carbon Intensity statistics are available for up to 30 days, by default 10 \"\"\" logger = prefect . context . get ( \"logger\" ) carbon = UKCarbonIntensity () now = datetime . datetime . now () logger . info ( f \"Downloading data to { path } ...\" ) for i in range ( days_back ): from_delta = datetime . timedelta ( days = i + 1 ) to_delta = datetime . timedelta ( days = i ) to = now - to_delta from_ = now - from_delta carbon . query ( f \"/intensity/stats/ { from_ . isoformat () } / { to . isoformat () } \" ) carbon . to_csv ( path , if_exists = \"append\" ) # Download data to a local CSV file logger . info ( f \"Successfully downloaded data to { path } .\" ) viadot.tasks.open_apis.uk_carbon_intensity.StatsToExcel A Prefect task for downloading UK Carbon Instensity Statistics (stats) to a excel file. __call__ ( self ) special Run the task. Parameters path : str Path of the csv file created or edited by this task days_back : int, optional How many days of stats to download in the excel. UK Carbon Intensity statistics are available for up to 30 days, by default 10 Source code in viadot/tasks/open_apis/uk_carbon_intensity.py def __call__ ( self ): \"\"\" Run the task. Parameters ---------- path : str Path of the csv file created or edited by this task days_back : int, optional How many days of stats to download in the excel. UK Carbon Intensity statistics are available for up to 30 days, by default 10 \"\"\" __init__ ( self , * args , ** kwargs ) special Generate the task. Source code in viadot/tasks/open_apis/uk_carbon_intensity.py def __init__ ( self , * args , ** kwargs ): \"\"\"Generate the task.\"\"\" super () . __init__ ( name = \"uk_carbon_intensity_stats_to_excel\" , * args , ** kwargs ) run ( self , path , days_back = 10 ) Run the task. Parameters path : str Path of the excel file created or edited by this task days_back : int, optional How many days of stats to download in the excel. UK Carbon Intensity statistics are available for up to 30 days, by default 10 Source code in viadot/tasks/open_apis/uk_carbon_intensity.py def run ( self , path : str , days_back : int = 10 ): \"\"\" Run the task. Parameters ---------- path : str Path of the excel file created or edited by this task days_back : int, optional How many days of stats to download in the excel. UK Carbon Intensity statistics are available for up to 30 days, by default 10 \"\"\" logger = prefect . context . get ( \"logger\" ) carbon = UKCarbonIntensity () now = datetime . datetime . now () logger . info ( f \"Downloading data to { path } ...\" ) for i in range ( days_back ): from_delta = datetime . timedelta ( days = i + 1 ) to_delta = datetime . timedelta ( days = i ) to = now - to_delta from_ = now - from_delta carbon . query ( f \"/intensity/stats/ { from_ . isoformat () } / { to . isoformat () } \" ) carbon . to_excel ( path , if_exists = \"append\" ) # Download data to a local excel file logger . info ( f \"Successfully downloaded data to { path } .\" )","title":"Tasks library"},{"location":"references/task_library/#viadot.tasks.open_apis.uk_carbon_intensity.StatsToCSV","text":"A Prefect task for downloading UK Carbon Instensity Statistics (stats) to a csv file.","title":"StatsToCSV"},{"location":"references/task_library/#viadot.tasks.open_apis.uk_carbon_intensity.StatsToCSV.__call__","text":"Run the task.","title":"__call__()"},{"location":"references/task_library/#viadot.tasks.open_apis.uk_carbon_intensity.StatsToCSV.__call__--parameters","text":"path : str Path of the csv file created or edited by this task days_back : int, optional How many days of stats to download in the csv. UK Carbon Intensity statistics are available for up to 30 days, by default 10 Source code in viadot/tasks/open_apis/uk_carbon_intensity.py def __call__ ( self ): \"\"\" Run the task. Parameters ---------- path : str Path of the csv file created or edited by this task days_back : int, optional How many days of stats to download in the csv. UK Carbon Intensity statistics are available for up to 30 days, by default 10 \"\"\"","title":"Parameters"},{"location":"references/task_library/#viadot.tasks.open_apis.uk_carbon_intensity.StatsToCSV.__init__","text":"Generate the task. Source code in viadot/tasks/open_apis/uk_carbon_intensity.py def __init__ ( self , * args , ** kwargs ): \"\"\"Generate the task.\"\"\" super () . __init__ ( name = \"uk_carbon_intensity_stats_to_csv\" , * args , ** kwargs )","title":"__init__()"},{"location":"references/task_library/#viadot.tasks.open_apis.uk_carbon_intensity.StatsToCSV.run","text":"Run the task.","title":"run()"},{"location":"references/task_library/#viadot.tasks.open_apis.uk_carbon_intensity.StatsToCSV.run--parameters","text":"path : str Path of the csv file created or edited by this task days_back : int, optional How many days of stats to download in the csv. UK Carbon Intensity statistics are available for up to 30 days, by default 10 Source code in viadot/tasks/open_apis/uk_carbon_intensity.py def run ( self , path : str , days_back : int = 10 ): \"\"\" Run the task. Parameters ---------- path : str Path of the csv file created or edited by this task days_back : int, optional How many days of stats to download in the csv. UK Carbon Intensity statistics are available for up to 30 days, by default 10 \"\"\" logger = prefect . context . get ( \"logger\" ) carbon = UKCarbonIntensity () now = datetime . datetime . now () logger . info ( f \"Downloading data to { path } ...\" ) for i in range ( days_back ): from_delta = datetime . timedelta ( days = i + 1 ) to_delta = datetime . timedelta ( days = i ) to = now - to_delta from_ = now - from_delta carbon . query ( f \"/intensity/stats/ { from_ . isoformat () } / { to . isoformat () } \" ) carbon . to_csv ( path , if_exists = \"append\" ) # Download data to a local CSV file logger . info ( f \"Successfully downloaded data to { path } .\" )","title":"Parameters"},{"location":"references/task_library/#viadot.tasks.open_apis.uk_carbon_intensity.StatsToExcel","text":"A Prefect task for downloading UK Carbon Instensity Statistics (stats) to a excel file.","title":"StatsToExcel"},{"location":"references/task_library/#viadot.tasks.open_apis.uk_carbon_intensity.StatsToExcel.__call__","text":"Run the task.","title":"__call__()"},{"location":"references/task_library/#viadot.tasks.open_apis.uk_carbon_intensity.StatsToExcel.__call__--parameters","text":"path : str Path of the csv file created or edited by this task days_back : int, optional How many days of stats to download in the excel. UK Carbon Intensity statistics are available for up to 30 days, by default 10 Source code in viadot/tasks/open_apis/uk_carbon_intensity.py def __call__ ( self ): \"\"\" Run the task. Parameters ---------- path : str Path of the csv file created or edited by this task days_back : int, optional How many days of stats to download in the excel. UK Carbon Intensity statistics are available for up to 30 days, by default 10 \"\"\"","title":"Parameters"},{"location":"references/task_library/#viadot.tasks.open_apis.uk_carbon_intensity.StatsToExcel.__init__","text":"Generate the task. Source code in viadot/tasks/open_apis/uk_carbon_intensity.py def __init__ ( self , * args , ** kwargs ): \"\"\"Generate the task.\"\"\" super () . __init__ ( name = \"uk_carbon_intensity_stats_to_excel\" , * args , ** kwargs )","title":"__init__()"},{"location":"references/task_library/#viadot.tasks.open_apis.uk_carbon_intensity.StatsToExcel.run","text":"Run the task.","title":"run()"},{"location":"references/task_library/#viadot.tasks.open_apis.uk_carbon_intensity.StatsToExcel.run--parameters","text":"path : str Path of the excel file created or edited by this task days_back : int, optional How many days of stats to download in the excel. UK Carbon Intensity statistics are available for up to 30 days, by default 10 Source code in viadot/tasks/open_apis/uk_carbon_intensity.py def run ( self , path : str , days_back : int = 10 ): \"\"\" Run the task. Parameters ---------- path : str Path of the excel file created or edited by this task days_back : int, optional How many days of stats to download in the excel. UK Carbon Intensity statistics are available for up to 30 days, by default 10 \"\"\" logger = prefect . context . get ( \"logger\" ) carbon = UKCarbonIntensity () now = datetime . datetime . now () logger . info ( f \"Downloading data to { path } ...\" ) for i in range ( days_back ): from_delta = datetime . timedelta ( days = i + 1 ) to_delta = datetime . timedelta ( days = i ) to = now - to_delta from_ = now - from_delta carbon . query ( f \"/intensity/stats/ { from_ . isoformat () } / { to . isoformat () } \" ) carbon . to_excel ( path , if_exists = \"append\" ) # Download data to a local excel file logger . info ( f \"Successfully downloaded data to { path } .\" )","title":"Parameters"},{"location":"tutorials/placeholder/","text":"TODO","title":"Tutorials"},{"location":"tutorials/placeholder/#todo","text":"","title":"TODO"}]}